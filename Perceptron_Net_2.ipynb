{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "Perceptron Net 2.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGGeRGB3SdWe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fd778beb-100d-41db-9172-7690854e5911"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import seaborn as sns\n",
        "sns.despine()\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "#from keras.layers import Merge\n",
        "from keras.layers import Concatenate\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.optimizers import RMSprop, Adam, SGD, Nadam\n",
        "from keras.layers.advanced_activations import *\n",
        "from keras.layers import Convolution1D, MaxPooling1D, AtrousConvolution1D\n",
        "from keras.layers.recurrent import LSTM, GRU\n",
        "from keras import regularizers\n",
        "\n",
        "import theano\n",
        "theano.config.compute_test_value = \"ignore\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dpk_l34zSdWf"
      },
      "source": [
        "def shuffle_in_unison(a, b):\n",
        "    # courtsey http://stackoverflow.com/users/190280/josh-bleecher-snyder\n",
        "    assert len(a) == len(b)\n",
        "    shuffled_a = np.empty(a.shape, dtype=a.dtype)\n",
        "    shuffled_b = np.empty(b.shape, dtype=b.dtype)\n",
        "    permutation = np.random.permutation(len(a))\n",
        "    for old_index, new_index in enumerate(permutation):\n",
        "        shuffled_a[new_index] = a[old_index]\n",
        "        shuffled_b[new_index] = b[old_index]\n",
        "    return shuffled_a, shuffled_b\n",
        " \n",
        "def create_Xt_Yt(X, y, percentage=0.9):\n",
        "    p = int(len(X) * percentage)\n",
        "    X_train = X[0:p]\n",
        "    Y_train = y[0:p]\n",
        "     \n",
        "    X_train, Y_train = shuffle_in_unison(X_train, Y_train)\n",
        " \n",
        "    X_test = X[p:]\n",
        "    Y_test = y[p:]\n",
        "\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL8mgVFISdWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "e4893be4-fff2-49ed-84ec-cf5987b29b93"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "drive.mount('/content/gdrive')\n",
        "data = pd.read_csv(open('/content/gdrive/My Drive/Нейросети/United dataframe.csv'))\n",
        "\n",
        "'''\n",
        "data = pd.read_csv('AAPL.csv')\n",
        "'''\n",
        "data = data.loc[:, '<CLAVRG>'].tolist()\n",
        "\n",
        "# Uncomment below to use price change time series\n",
        "# data = data.ix[:, 'Adj Close'].pct_change().dropna().tolist()\n",
        "\n",
        "plt.plot(data)\n",
        "plt.show()\n",
        "\n",
        "WINDOW = 30\n",
        "EMB_SIZE = 1\n",
        "STEP = 1\n",
        "FORECAST = 10\n",
        "\n",
        "# Straightforward way for creating time windows\n",
        "X, Y = [], []\n",
        "for i in range(0, len(data), STEP): \n",
        "    try:\n",
        "        x_i = data[i:i+WINDOW]\n",
        "        y_i = data[i+WINDOW+FORECAST]  \n",
        "\n",
        "        last_close = x_i[WINDOW-1]\n",
        "        next_close = y_i\n",
        "\n",
        "        if last_close < next_close:\n",
        "            y_i = [1, 0]\n",
        "        else:\n",
        "            y_i = [0, 1] \n",
        "\n",
        "    except Exception as e:\n",
        "        print e\n",
        "        break\n",
        "\n",
        "    X.append(x_i)\n",
        "    Y.append(y_i)\n",
        "\n",
        "X = [(np.array(x) - np.mean(x)) / np.std(x) for x in X] # comment it to remove normalization\n",
        "X, Y = np.array(X), np.array(Y)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = create_Xt_Yt(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKny1APuSdWf"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=30,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(128,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(32,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dense(16,\n",
        "                activity_regularizer=regularizers.l2(0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(LeakyReLU())\n",
        "\n",
        "model.add(Dense(2))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "opt = Nadam(lr=0.001)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=25, min_lr=0.000001, verbose=1)\n",
        "checkpointer = ModelCheckpoint(filepath=\"test.hdf5\", verbose=1, save_best_only=True)\n",
        "model.compile(optimizer=opt, \n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgXUXR1pSdWf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0705f8f9-4520-4bd0-b774-ea33ac1c89a4"
      },
      "source": [
        "history = model.fit(X_train, Y_train, \n",
        "          nb_epoch = 750, \n",
        "          batch_size = 256, \n",
        "          verbose=1, \n",
        "          validation_data=(X_test, Y_test),\n",
        "          callbacks=[reduce_lr, checkpointer],\n",
        "          shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 9809 samples, validate on 1090 samples\n",
            "Epoch 1/750\n",
            "9809/9809 [==============================] - 2s 248us/step - loss: 754.1950 - accuracy: 0.4917 - val_loss: 71.8094 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 71.80944, saving model to test.hdf5\n",
            "Epoch 2/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 440.8239 - accuracy: 0.5062 - val_loss: 45.1990 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00002: val_loss improved from 71.80944 to 45.19903, saving model to test.hdf5\n",
            "Epoch 3/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 282.5669 - accuracy: 0.4980 - val_loss: 33.1455 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00003: val_loss improved from 45.19903 to 33.14552, saving model to test.hdf5\n",
            "Epoch 4/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 188.5068 - accuracy: 0.5121 - val_loss: 25.8108 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00004: val_loss improved from 33.14552 to 25.81080, saving model to test.hdf5\n",
            "Epoch 5/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 129.8757 - accuracy: 0.5222 - val_loss: 20.8856 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00005: val_loss improved from 25.81080 to 20.88563, saving model to test.hdf5\n",
            "Epoch 6/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 91.6253 - accuracy: 0.5270 - val_loss: 17.5452 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00006: val_loss improved from 20.88563 to 17.54524, saving model to test.hdf5\n",
            "Epoch 7/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 66.1414 - accuracy: 0.5201 - val_loss: 14.9694 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00007: val_loss improved from 17.54524 to 14.96937, saving model to test.hdf5\n",
            "Epoch 8/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 48.6127 - accuracy: 0.5224 - val_loss: 12.9958 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00008: val_loss improved from 14.96937 to 12.99585, saving model to test.hdf5\n",
            "Epoch 9/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 36.4303 - accuracy: 0.5286 - val_loss: 11.3781 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00009: val_loss improved from 12.99585 to 11.37807, saving model to test.hdf5\n",
            "Epoch 10/750\n",
            "9809/9809 [==============================] - 0s 40us/step - loss: 27.6144 - accuracy: 0.5297 - val_loss: 10.0648 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00010: val_loss improved from 11.37807 to 10.06483, saving model to test.hdf5\n",
            "Epoch 11/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 21.2182 - accuracy: 0.5277 - val_loss: 8.9611 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00011: val_loss improved from 10.06483 to 8.96113, saving model to test.hdf5\n",
            "Epoch 12/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 16.5350 - accuracy: 0.5245 - val_loss: 8.0137 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00012: val_loss improved from 8.96113 to 8.01373, saving model to test.hdf5\n",
            "Epoch 13/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 13.0753 - accuracy: 0.5335 - val_loss: 7.2063 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00013: val_loss improved from 8.01373 to 7.20629, saving model to test.hdf5\n",
            "Epoch 14/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 10.4486 - accuracy: 0.5302 - val_loss: 6.5050 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00014: val_loss improved from 7.20629 to 6.50504, saving model to test.hdf5\n",
            "Epoch 15/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 8.4728 - accuracy: 0.5246 - val_loss: 5.9069 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00015: val_loss improved from 6.50504 to 5.90694, saving model to test.hdf5\n",
            "Epoch 16/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 6.9748 - accuracy: 0.5327 - val_loss: 5.3701 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00016: val_loss improved from 5.90694 to 5.37013, saving model to test.hdf5\n",
            "Epoch 17/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 5.8570 - accuracy: 0.5332 - val_loss: 4.8969 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00017: val_loss improved from 5.37013 to 4.89690, saving model to test.hdf5\n",
            "Epoch 18/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 5.0141 - accuracy: 0.5354 - val_loss: 4.4801 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00018: val_loss improved from 4.89690 to 4.48012, saving model to test.hdf5\n",
            "Epoch 19/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 4.3751 - accuracy: 0.5337 - val_loss: 4.1082 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00019: val_loss improved from 4.48012 to 4.10820, saving model to test.hdf5\n",
            "Epoch 20/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 3.8895 - accuracy: 0.5311 - val_loss: 3.7754 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00020: val_loss improved from 4.10820 to 3.77539, saving model to test.hdf5\n",
            "Epoch 21/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 3.5009 - accuracy: 0.5374 - val_loss: 3.4815 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00021: val_loss improved from 3.77539 to 3.48146, saving model to test.hdf5\n",
            "Epoch 22/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 3.1872 - accuracy: 0.5369 - val_loss: 3.2201 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00022: val_loss improved from 3.48146 to 3.22009, saving model to test.hdf5\n",
            "Epoch 23/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 2.9219 - accuracy: 0.5369 - val_loss: 2.9856 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00023: val_loss improved from 3.22009 to 2.98559, saving model to test.hdf5\n",
            "Epoch 24/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 2.6965 - accuracy: 0.5386 - val_loss: 2.7741 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00024: val_loss improved from 2.98559 to 2.77409, saving model to test.hdf5\n",
            "Epoch 25/750\n",
            "9809/9809 [==============================] - 0s 40us/step - loss: 2.4996 - accuracy: 0.5375 - val_loss: 2.5826 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00025: val_loss improved from 2.77409 to 2.58260, saving model to test.hdf5\n",
            "Epoch 26/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 2.3221 - accuracy: 0.5369 - val_loss: 2.4081 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.000900000042748.\n",
            "\n",
            "Epoch 00026: val_loss improved from 2.58260 to 2.40807, saving model to test.hdf5\n",
            "Epoch 27/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 2.1713 - accuracy: 0.5374 - val_loss: 2.2634 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00027: val_loss improved from 2.40807 to 2.26343, saving model to test.hdf5\n",
            "Epoch 28/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 2.0445 - accuracy: 0.5365 - val_loss: 2.1303 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00028: val_loss improved from 2.26343 to 2.13029, saving model to test.hdf5\n",
            "Epoch 29/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 1.9263 - accuracy: 0.5365 - val_loss: 2.0069 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00029: val_loss improved from 2.13029 to 2.00690, saving model to test.hdf5\n",
            "Epoch 30/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 1.8169 - accuracy: 0.5372 - val_loss: 1.8929 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00030: val_loss improved from 2.00690 to 1.89292, saving model to test.hdf5\n",
            "Epoch 31/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 1.7175 - accuracy: 0.5369 - val_loss: 1.7876 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00031: val_loss improved from 1.89292 to 1.78755, saving model to test.hdf5\n",
            "Epoch 32/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 1.6263 - accuracy: 0.5371 - val_loss: 1.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00032: val_loss improved from 1.78755 to 1.68987, saving model to test.hdf5\n",
            "Epoch 33/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 1.5410 - accuracy: 0.5368 - val_loss: 1.5999 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00033: val_loss improved from 1.68987 to 1.59994, saving model to test.hdf5\n",
            "Epoch 34/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 1.4642 - accuracy: 0.5370 - val_loss: 1.5167 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00034: val_loss improved from 1.59994 to 1.51673, saving model to test.hdf5\n",
            "Epoch 35/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 1.3932 - accuracy: 0.5370 - val_loss: 1.4402 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00035: val_loss improved from 1.51673 to 1.44018, saving model to test.hdf5\n",
            "Epoch 36/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 1.3268 - accuracy: 0.5370 - val_loss: 1.3702 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00036: val_loss improved from 1.44018 to 1.37018, saving model to test.hdf5\n",
            "Epoch 37/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 1.2671 - accuracy: 0.5369 - val_loss: 1.3051 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00037: val_loss improved from 1.37018 to 1.30512, saving model to test.hdf5\n",
            "Epoch 38/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 1.2113 - accuracy: 0.5370 - val_loss: 1.2456 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00038: val_loss improved from 1.30512 to 1.24561, saving model to test.hdf5\n",
            "Epoch 39/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 1.1609 - accuracy: 0.5371 - val_loss: 1.1915 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00039: val_loss improved from 1.24561 to 1.19154, saving model to test.hdf5\n",
            "Epoch 40/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 1.1147 - accuracy: 0.5370 - val_loss: 1.1414 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00040: val_loss improved from 1.19154 to 1.14145, saving model to test.hdf5\n",
            "Epoch 41/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 1.0721 - accuracy: 0.5370 - val_loss: 1.0959 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00041: val_loss improved from 1.14145 to 1.09591, saving model to test.hdf5\n",
            "Epoch 42/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 1.0340 - accuracy: 0.5370 - val_loss: 1.0544 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00042: val_loss improved from 1.09591 to 1.05437, saving model to test.hdf5\n",
            "Epoch 43/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.9987 - accuracy: 0.5370 - val_loss: 1.0166 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00043: val_loss improved from 1.05437 to 1.01663, saving model to test.hdf5\n",
            "Epoch 44/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.9668 - accuracy: 0.5370 - val_loss: 0.9822 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00044: val_loss improved from 1.01663 to 0.98225, saving model to test.hdf5\n",
            "Epoch 45/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.9376 - accuracy: 0.5369 - val_loss: 0.9511 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.98225 to 0.95110, saving model to test.hdf5\n",
            "Epoch 46/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.9116 - accuracy: 0.5369 - val_loss: 0.9229 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.95110 to 0.92288, saving model to test.hdf5\n",
            "Epoch 47/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.8878 - accuracy: 0.5370 - val_loss: 0.8973 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.92288 to 0.89732, saving model to test.hdf5\n",
            "Epoch 48/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.8664 - accuracy: 0.5370 - val_loss: 0.8743 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.89732 to 0.87427, saving model to test.hdf5\n",
            "Epoch 49/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.8466 - accuracy: 0.5370 - val_loss: 0.8535 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.87427 to 0.85349, saving model to test.hdf5\n",
            "Epoch 50/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.8296 - accuracy: 0.5370 - val_loss: 0.8348 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.85349 to 0.83481, saving model to test.hdf5\n",
            "Epoch 51/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.8139 - accuracy: 0.5370 - val_loss: 0.8180 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00051: ReduceLROnPlateau reducing learning rate to 0.000810000038473.\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.83481 to 0.81802, saving model to test.hdf5\n",
            "Epoch 52/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.8005 - accuracy: 0.5370 - val_loss: 0.8044 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.81802 to 0.80438, saving model to test.hdf5\n",
            "Epoch 53/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.7890 - accuracy: 0.5370 - val_loss: 0.7921 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.80438 to 0.79211, saving model to test.hdf5\n",
            "Epoch 54/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.7786 - accuracy: 0.5370 - val_loss: 0.7810 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.79211 to 0.78096, saving model to test.hdf5\n",
            "Epoch 55/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.7691 - accuracy: 0.5370 - val_loss: 0.7709 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.78096 to 0.77086, saving model to test.hdf5\n",
            "Epoch 56/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.7607 - accuracy: 0.5370 - val_loss: 0.7617 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.77086 to 0.76173, saving model to test.hdf5\n",
            "Epoch 57/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7530 - accuracy: 0.5370 - val_loss: 0.7535 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.76173 to 0.75353, saving model to test.hdf5\n",
            "Epoch 58/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7460 - accuracy: 0.5370 - val_loss: 0.7462 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.75353 to 0.74621, saving model to test.hdf5\n",
            "Epoch 59/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.7398 - accuracy: 0.5370 - val_loss: 0.7396 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.74621 to 0.73963, saving model to test.hdf5\n",
            "Epoch 60/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7344 - accuracy: 0.5370 - val_loss: 0.7338 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.73963 to 0.73376, saving model to test.hdf5\n",
            "Epoch 61/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.7293 - accuracy: 0.5370 - val_loss: 0.7285 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.73376 to 0.72848, saving model to test.hdf5\n",
            "Epoch 62/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 0.7250 - accuracy: 0.5370 - val_loss: 0.7238 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.72848 to 0.72379, saving model to test.hdf5\n",
            "Epoch 63/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.7208 - accuracy: 0.5370 - val_loss: 0.7196 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.72379 to 0.71963, saving model to test.hdf5\n",
            "Epoch 64/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.7173 - accuracy: 0.5370 - val_loss: 0.7159 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.71963 to 0.71589, saving model to test.hdf5\n",
            "Epoch 65/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.7144 - accuracy: 0.5370 - val_loss: 0.7126 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.71589 to 0.71261, saving model to test.hdf5\n",
            "Epoch 66/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7115 - accuracy: 0.5370 - val_loss: 0.7097 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.71261 to 0.70975, saving model to test.hdf5\n",
            "Epoch 67/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.7091 - accuracy: 0.5370 - val_loss: 0.7071 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.70975 to 0.70713, saving model to test.hdf5\n",
            "Epoch 68/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.7069 - accuracy: 0.5370 - val_loss: 0.7049 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.70713 to 0.70490, saving model to test.hdf5\n",
            "Epoch 69/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.7050 - accuracy: 0.5370 - val_loss: 0.7029 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.70490 to 0.70293, saving model to test.hdf5\n",
            "Epoch 70/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7033 - accuracy: 0.5370 - val_loss: 0.7012 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.70293 to 0.70117, saving model to test.hdf5\n",
            "Epoch 71/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.7019 - accuracy: 0.5370 - val_loss: 0.6996 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.70117 to 0.69965, saving model to test.hdf5\n",
            "Epoch 72/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.7005 - accuracy: 0.5370 - val_loss: 0.6983 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.69965 to 0.69833, saving model to test.hdf5\n",
            "Epoch 73/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6994 - accuracy: 0.5370 - val_loss: 0.6972 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.69833 to 0.69715, saving model to test.hdf5\n",
            "Epoch 74/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6982 - accuracy: 0.5370 - val_loss: 0.6961 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00074: val_loss improved from 0.69715 to 0.69611, saving model to test.hdf5\n",
            "Epoch 75/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6975 - accuracy: 0.5370 - val_loss: 0.6952 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00075: val_loss improved from 0.69611 to 0.69524, saving model to test.hdf5\n",
            "Epoch 76/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6968 - accuracy: 0.5370 - val_loss: 0.6945 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00076: ReduceLROnPlateau reducing learning rate to 0.000729000050342.\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.69524 to 0.69446, saving model to test.hdf5\n",
            "Epoch 77/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6960 - accuracy: 0.5370 - val_loss: 0.6939 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.69446 to 0.69386, saving model to test.hdf5\n",
            "Epoch 78/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6956 - accuracy: 0.5370 - val_loss: 0.6933 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.69386 to 0.69334, saving model to test.hdf5\n",
            "Epoch 79/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6950 - accuracy: 0.5370 - val_loss: 0.6929 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00079: val_loss improved from 0.69334 to 0.69288, saving model to test.hdf5\n",
            "Epoch 80/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6947 - accuracy: 0.5370 - val_loss: 0.6925 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.69288 to 0.69247, saving model to test.hdf5\n",
            "Epoch 81/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6942 - accuracy: 0.5370 - val_loss: 0.6921 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00081: val_loss improved from 0.69247 to 0.69212, saving model to test.hdf5\n",
            "Epoch 82/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6940 - accuracy: 0.5370 - val_loss: 0.6918 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00082: val_loss improved from 0.69212 to 0.69184, saving model to test.hdf5\n",
            "Epoch 83/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6938 - accuracy: 0.5370 - val_loss: 0.6916 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.69184 to 0.69159, saving model to test.hdf5\n",
            "Epoch 84/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6934 - accuracy: 0.5370 - val_loss: 0.6913 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.69159 to 0.69134, saving model to test.hdf5\n",
            "Epoch 85/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6932 - accuracy: 0.5370 - val_loss: 0.6911 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.69134 to 0.69114, saving model to test.hdf5\n",
            "Epoch 86/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6930 - accuracy: 0.5370 - val_loss: 0.6910 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.69114 to 0.69097, saving model to test.hdf5\n",
            "Epoch 87/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6929 - accuracy: 0.5370 - val_loss: 0.6908 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00087: val_loss improved from 0.69097 to 0.69082, saving model to test.hdf5\n",
            "Epoch 88/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6926 - accuracy: 0.5370 - val_loss: 0.6907 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00088: val_loss improved from 0.69082 to 0.69069, saving model to test.hdf5\n",
            "Epoch 89/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6926 - accuracy: 0.5370 - val_loss: 0.6906 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.69069 to 0.69058, saving model to test.hdf5\n",
            "Epoch 90/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6924 - accuracy: 0.5370 - val_loss: 0.6905 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00090: val_loss improved from 0.69058 to 0.69049, saving model to test.hdf5\n",
            "Epoch 91/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6923 - accuracy: 0.5370 - val_loss: 0.6904 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.69049 to 0.69038, saving model to test.hdf5\n",
            "Epoch 92/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6923 - accuracy: 0.5370 - val_loss: 0.6903 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00092: val_loss improved from 0.69038 to 0.69031, saving model to test.hdf5\n",
            "Epoch 93/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6922 - accuracy: 0.5370 - val_loss: 0.6902 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00093: val_loss improved from 0.69031 to 0.69024, saving model to test.hdf5\n",
            "Epoch 94/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6921 - accuracy: 0.5370 - val_loss: 0.6902 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.69024 to 0.69021, saving model to test.hdf5\n",
            "Epoch 95/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6920 - accuracy: 0.5370 - val_loss: 0.6902 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.69021 to 0.69019, saving model to test.hdf5\n",
            "Epoch 96/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6919 - accuracy: 0.5370 - val_loss: 0.6902 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.69019 to 0.69017, saving model to test.hdf5\n",
            "Epoch 97/750\n",
            "9809/9809 [==============================] - 0s 41us/step - loss: 0.6920 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.69017 to 0.69011, saving model to test.hdf5\n",
            "Epoch 98/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6919 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.69011 to 0.69007, saving model to test.hdf5\n",
            "Epoch 99/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6919 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.69007 to 0.69006, saving model to test.hdf5\n",
            "Epoch 100/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6918 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.69006 to 0.69005, saving model to test.hdf5\n",
            "Epoch 101/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6918 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00101: ReduceLROnPlateau reducing learning rate to 0.000656100071501.\n",
            "\n",
            "Epoch 00101: val_loss improved from 0.69005 to 0.69003, saving model to test.hdf5\n",
            "Epoch 102/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6918 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00102: val_loss improved from 0.69003 to 0.68999, saving model to test.hdf5\n",
            "Epoch 103/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6917 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00103: val_loss improved from 0.68999 to 0.68999, saving model to test.hdf5\n",
            "Epoch 104/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6917 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.68999\n",
            "Epoch 105/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6917 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00105: val_loss improved from 0.68999 to 0.68998, saving model to test.hdf5\n",
            "Epoch 106/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6916 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00106: val_loss improved from 0.68998 to 0.68997, saving model to test.hdf5\n",
            "Epoch 107/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6916 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00107: val_loss improved from 0.68997 to 0.68995, saving model to test.hdf5\n",
            "Epoch 108/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6915 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00108: val_loss improved from 0.68995 to 0.68994, saving model to test.hdf5\n",
            "Epoch 109/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6915 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00109: val_loss improved from 0.68994 to 0.68993, saving model to test.hdf5\n",
            "Epoch 110/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6915 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.68993\n",
            "Epoch 111/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6915 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.68993\n",
            "Epoch 112/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.68993\n",
            "Epoch 113/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.68993\n",
            "Epoch 114/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.68993\n",
            "Epoch 115/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00115: val_loss improved from 0.68993 to 0.68993, saving model to test.hdf5\n",
            "Epoch 116/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.68993\n",
            "Epoch 117/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.68993\n",
            "Epoch 118/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6914 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.68993\n",
            "Epoch 119/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00119: val_loss improved from 0.68993 to 0.68993, saving model to test.hdf5\n",
            "Epoch 120/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00120: val_loss improved from 0.68993 to 0.68991, saving model to test.hdf5\n",
            "Epoch 121/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.68991\n",
            "Epoch 122/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.68991\n",
            "Epoch 123/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00123: val_loss improved from 0.68991 to 0.68990, saving model to test.hdf5\n",
            "Epoch 124/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.68990\n",
            "Epoch 125/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00125: val_loss did not improve from 0.68990\n",
            "Epoch 126/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6913 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00126: ReduceLROnPlateau reducing learning rate to 0.000590490043396.\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.68990\n",
            "Epoch 127/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.68990\n",
            "Epoch 128/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.68990\n",
            "Epoch 129/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.68990\n",
            "Epoch 130/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.68990\n",
            "Epoch 131/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.68990\n",
            "Epoch 132/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6912 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.68990\n",
            "Epoch 133/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00133: val_loss improved from 0.68990 to 0.68990, saving model to test.hdf5\n",
            "Epoch 134/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.68990\n",
            "Epoch 135/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.68990\n",
            "Epoch 136/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.68990\n",
            "Epoch 137/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.68990\n",
            "Epoch 138/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.68990\n",
            "Epoch 139/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6911 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.68990\n",
            "Epoch 140/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.68990\n",
            "Epoch 141/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.68990\n",
            "Epoch 142/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.68990\n",
            "Epoch 143/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.68990\n",
            "Epoch 144/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.68990\n",
            "Epoch 145/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.68990\n",
            "Epoch 146/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.68990\n",
            "Epoch 147/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.68990\n",
            "Epoch 148/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00148: val_loss improved from 0.68990 to 0.68990, saving model to test.hdf5\n",
            "Epoch 149/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.68990\n",
            "Epoch 150/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6910 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.68990\n",
            "Epoch 151/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00151: ReduceLROnPlateau reducing learning rate to 0.000531441054773.\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.68990\n",
            "Epoch 152/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.68990\n",
            "Epoch 153/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.68990\n",
            "Epoch 154/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.68990\n",
            "Epoch 155/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.68990\n",
            "Epoch 156/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.68990\n",
            "Epoch 157/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.68990\n",
            "Epoch 158/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.68990\n",
            "Epoch 159/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.68990\n",
            "Epoch 160/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.68990\n",
            "Epoch 161/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.68990\n",
            "Epoch 162/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.68990\n",
            "Epoch 163/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.68990\n",
            "Epoch 164/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6909 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.68990\n",
            "Epoch 165/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.68990\n",
            "Epoch 166/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.68990\n",
            "Epoch 167/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.68990\n",
            "Epoch 168/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.68990\n",
            "Epoch 169/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.68990\n",
            "Epoch 170/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.68990\n",
            "Epoch 171/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.68990\n",
            "Epoch 172/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.68990\n",
            "Epoch 173/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00173: val_loss improved from 0.68990 to 0.68990, saving model to test.hdf5\n",
            "Epoch 174/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.68990\n",
            "Epoch 175/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.68990\n",
            "Epoch 176/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00176: ReduceLROnPlateau reducing learning rate to 0.000478296959773.\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.68990\n",
            "Epoch 177/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.68990\n",
            "Epoch 178/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.68990\n",
            "Epoch 179/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.68990\n",
            "Epoch 180/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.68990\n",
            "Epoch 181/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.68990\n",
            "Epoch 182/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.68990\n",
            "Epoch 183/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.68990\n",
            "Epoch 184/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.68990\n",
            "Epoch 185/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.68990\n",
            "Epoch 186/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.68990\n",
            "Epoch 187/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.68990\n",
            "Epoch 188/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.68990\n",
            "Epoch 189/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.68990\n",
            "Epoch 190/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.68990\n",
            "Epoch 191/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.68990\n",
            "Epoch 192/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.68990\n",
            "Epoch 193/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.68990\n",
            "Epoch 194/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.68990\n",
            "Epoch 195/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.68990\n",
            "Epoch 196/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.68990\n",
            "Epoch 197/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.68990\n",
            "Epoch 198/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.68990\n",
            "Epoch 199/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.68990\n",
            "Epoch 200/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.68990\n",
            "Epoch 201/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00201: ReduceLROnPlateau reducing learning rate to 0.000430467253318.\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.68990\n",
            "Epoch 202/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.68990\n",
            "Epoch 203/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.68990\n",
            "Epoch 204/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.68990\n",
            "Epoch 205/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.68990\n",
            "Epoch 206/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.68990\n",
            "Epoch 207/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.68990\n",
            "Epoch 208/750\n",
            "9809/9809 [==============================] - 0s 42us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.68990\n",
            "Epoch 209/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.68990\n",
            "Epoch 210/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.68990\n",
            "Epoch 211/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.68990\n",
            "Epoch 212/750\n",
            "9809/9809 [==============================] - 0s 43us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.68990\n",
            "Epoch 213/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.68990\n",
            "Epoch 214/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.68990\n",
            "Epoch 215/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.68990\n",
            "Epoch 216/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.68990\n",
            "Epoch 217/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.68990\n",
            "Epoch 218/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.68990\n",
            "Epoch 219/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.68990\n",
            "Epoch 220/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.68990\n",
            "Epoch 221/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.68990\n",
            "Epoch 222/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.68990\n",
            "Epoch 223/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.68990\n",
            "Epoch 224/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.68990\n",
            "Epoch 225/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.68990\n",
            "Epoch 226/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00226: ReduceLROnPlateau reducing learning rate to 0.000387420522748.\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.68990\n",
            "Epoch 227/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.68990\n",
            "Epoch 228/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.68990\n",
            "Epoch 229/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.68990\n",
            "Epoch 230/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.68990\n",
            "Epoch 231/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.68990\n",
            "Epoch 232/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.68990\n",
            "Epoch 233/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.68990\n",
            "Epoch 234/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.68990\n",
            "Epoch 235/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.68990\n",
            "Epoch 236/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.68990\n",
            "Epoch 237/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.68990\n",
            "Epoch 238/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.68990\n",
            "Epoch 239/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.68990\n",
            "Epoch 240/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.68990\n",
            "Epoch 241/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.68990\n",
            "Epoch 242/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.68990\n",
            "Epoch 243/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.68990\n",
            "Epoch 244/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.68990\n",
            "Epoch 245/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.68990\n",
            "Epoch 246/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.68990\n",
            "Epoch 247/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.68990\n",
            "Epoch 248/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.68990\n",
            "Epoch 249/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.68990\n",
            "Epoch 250/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.68990\n",
            "Epoch 251/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00251: ReduceLROnPlateau reducing learning rate to 0.000348678475711.\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.68990\n",
            "Epoch 252/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.68990\n",
            "Epoch 253/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.68990\n",
            "Epoch 254/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.68990\n",
            "Epoch 255/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.68990\n",
            "Epoch 256/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.68990\n",
            "Epoch 257/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.68990\n",
            "Epoch 258/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.68990\n",
            "Epoch 259/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.68990\n",
            "Epoch 260/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.68990\n",
            "Epoch 261/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.68990\n",
            "Epoch 262/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.68990\n",
            "Epoch 263/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.68990\n",
            "Epoch 264/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.68990\n",
            "Epoch 265/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.68990\n",
            "Epoch 266/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.68990\n",
            "Epoch 267/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.68990\n",
            "Epoch 268/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.68990\n",
            "Epoch 269/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.68990\n",
            "Epoch 270/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.68990\n",
            "Epoch 271/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.68990\n",
            "Epoch 272/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.68990\n",
            "Epoch 273/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.68990\n",
            "Epoch 274/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.68990\n",
            "Epoch 275/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.68990\n",
            "Epoch 276/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00276: ReduceLROnPlateau reducing learning rate to 0.000313810622902.\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.68990\n",
            "Epoch 277/750\n",
            "9809/9809 [==============================] - 0s 44us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.68990\n",
            "Epoch 278/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.68990\n",
            "Epoch 279/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.68990\n",
            "Epoch 280/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.68990\n",
            "Epoch 281/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.68990\n",
            "Epoch 282/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.68990\n",
            "Epoch 283/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.68990\n",
            "Epoch 284/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.68990\n",
            "Epoch 285/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.68990\n",
            "Epoch 286/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.68990\n",
            "Epoch 287/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.68990\n",
            "Epoch 288/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.68990\n",
            "Epoch 289/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.68990\n",
            "Epoch 290/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.68990\n",
            "Epoch 291/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.68990\n",
            "Epoch 292/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.68990\n",
            "Epoch 293/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.68990\n",
            "Epoch 294/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.68990\n",
            "Epoch 295/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.68990\n",
            "Epoch 296/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.68990\n",
            "Epoch 297/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.68990\n",
            "Epoch 298/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.68990\n",
            "Epoch 299/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.68990\n",
            "Epoch 300/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.68990\n",
            "Epoch 301/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00301: ReduceLROnPlateau reducing learning rate to 0.000282429563231.\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.68990\n",
            "Epoch 302/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.68990\n",
            "Epoch 303/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.68990\n",
            "Epoch 304/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.68990\n",
            "Epoch 305/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.68990\n",
            "Epoch 306/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.68990\n",
            "Epoch 307/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.68990\n",
            "Epoch 308/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.68990\n",
            "Epoch 309/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.68990\n",
            "Epoch 310/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.68990\n",
            "Epoch 311/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.68990\n",
            "Epoch 312/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.68990\n",
            "Epoch 313/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.68990\n",
            "Epoch 314/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.68990\n",
            "Epoch 315/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.68990\n",
            "Epoch 316/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.68990\n",
            "Epoch 317/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.68990\n",
            "Epoch 318/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.68990\n",
            "Epoch 319/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.68990\n",
            "Epoch 320/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.68990\n",
            "Epoch 321/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.68990\n",
            "Epoch 322/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.68990\n",
            "Epoch 323/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.68990\n",
            "Epoch 324/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.68990\n",
            "Epoch 325/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.68990\n",
            "Epoch 326/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00326: ReduceLROnPlateau reducing learning rate to 0.000254186609527.\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.68990\n",
            "Epoch 327/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.68990\n",
            "Epoch 328/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.68990\n",
            "Epoch 329/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.68990\n",
            "Epoch 330/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.68990\n",
            "Epoch 331/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.68990\n",
            "Epoch 332/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.68990\n",
            "Epoch 333/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.68990\n",
            "Epoch 334/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.68990\n",
            "Epoch 335/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.68990\n",
            "Epoch 336/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.68990\n",
            "Epoch 337/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.68990\n",
            "Epoch 338/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.68990\n",
            "Epoch 339/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.68990\n",
            "Epoch 340/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.68990\n",
            "Epoch 341/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.68990\n",
            "Epoch 342/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.68990\n",
            "Epoch 343/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.68990\n",
            "Epoch 344/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.68990\n",
            "Epoch 345/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.68990\n",
            "Epoch 346/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.68990\n",
            "Epoch 347/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.68990\n",
            "Epoch 348/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.68990\n",
            "Epoch 349/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.68990\n",
            "Epoch 350/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.68990\n",
            "Epoch 351/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00351: ReduceLROnPlateau reducing learning rate to 0.000228767938097.\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.68990\n",
            "Epoch 352/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.68990\n",
            "Epoch 353/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.68990\n",
            "Epoch 354/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.68990\n",
            "Epoch 355/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.68990\n",
            "Epoch 356/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.68990\n",
            "Epoch 357/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.68990\n",
            "Epoch 358/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.68990\n",
            "Epoch 359/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.68990\n",
            "Epoch 360/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.68990\n",
            "Epoch 361/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.68990\n",
            "Epoch 362/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.68990\n",
            "Epoch 363/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.68990\n",
            "Epoch 364/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.68990\n",
            "Epoch 365/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.68990\n",
            "Epoch 366/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.68990\n",
            "Epoch 367/750\n",
            "9809/9809 [==============================] - 0s 45us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.68990\n",
            "Epoch 368/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.68990\n",
            "Epoch 369/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.68990\n",
            "Epoch 370/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.68990\n",
            "Epoch 371/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.68990\n",
            "Epoch 372/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.68990\n",
            "Epoch 373/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.68990\n",
            "Epoch 374/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.68990\n",
            "Epoch 375/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.68990\n",
            "Epoch 376/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00376: ReduceLROnPlateau reducing learning rate to 0.000205891144287.\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.68990\n",
            "Epoch 377/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.68990\n",
            "Epoch 378/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.68990\n",
            "Epoch 379/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.68990\n",
            "Epoch 380/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.68990\n",
            "Epoch 381/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.68990\n",
            "Epoch 382/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.68990\n",
            "Epoch 383/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.68990\n",
            "Epoch 384/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.68990\n",
            "Epoch 385/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.68990\n",
            "Epoch 386/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6908 - accuracy: 0.5370 - val_loss: 0.6901 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.68990\n",
            "Epoch 387/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.68990\n",
            "Epoch 388/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6907 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.68990\n",
            "Epoch 389/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.68990\n",
            "Epoch 390/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.68990\n",
            "Epoch 391/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.68990\n",
            "Epoch 392/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.68990\n",
            "Epoch 393/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.68990\n",
            "Epoch 394/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.68990\n",
            "Epoch 395/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.68990\n",
            "Epoch 396/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.68990\n",
            "Epoch 397/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.68990\n",
            "Epoch 398/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.68990\n",
            "Epoch 399/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.68990\n",
            "Epoch 400/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.68990\n",
            "Epoch 401/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00401: ReduceLROnPlateau reducing learning rate to 0.000185302033788.\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.68990\n",
            "Epoch 402/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.68990\n",
            "Epoch 403/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.68990\n",
            "Epoch 404/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.68990\n",
            "Epoch 405/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.68990\n",
            "Epoch 406/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.68990\n",
            "Epoch 407/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.68990\n",
            "Epoch 408/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.68990\n",
            "Epoch 409/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.68990\n",
            "Epoch 410/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.68990\n",
            "Epoch 411/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6906 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.68990\n",
            "Epoch 412/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.68990\n",
            "Epoch 413/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.68990\n",
            "Epoch 414/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.68990\n",
            "Epoch 415/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.68990\n",
            "Epoch 416/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.68990\n",
            "Epoch 417/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.68990\n",
            "Epoch 418/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.68990\n",
            "Epoch 419/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.68990\n",
            "Epoch 420/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.68990\n",
            "Epoch 421/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.68990\n",
            "Epoch 422/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.68990\n",
            "Epoch 423/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.68990\n",
            "Epoch 424/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.68990\n",
            "Epoch 425/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.68990\n",
            "Epoch 426/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00426: ReduceLROnPlateau reducing learning rate to 0.000166771833028.\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.68990\n",
            "Epoch 427/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.68990\n",
            "Epoch 428/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.68990\n",
            "Epoch 429/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.68990\n",
            "Epoch 430/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.68990\n",
            "Epoch 431/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.68990\n",
            "Epoch 432/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.68990\n",
            "Epoch 433/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.68990\n",
            "Epoch 434/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.68990\n",
            "Epoch 435/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.68990\n",
            "Epoch 436/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.68990\n",
            "Epoch 437/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.68990\n",
            "Epoch 438/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.68990\n",
            "Epoch 439/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.68990\n",
            "Epoch 440/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.68990\n",
            "Epoch 441/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.68990\n",
            "Epoch 442/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.68990\n",
            "Epoch 443/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.68990\n",
            "Epoch 444/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.68990\n",
            "Epoch 445/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.68990\n",
            "Epoch 446/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.68990\n",
            "Epoch 447/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.68990\n",
            "Epoch 448/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.68990\n",
            "Epoch 449/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.68990\n",
            "Epoch 450/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.68990\n",
            "Epoch 451/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00451: ReduceLROnPlateau reducing learning rate to 0.000150094648416.\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.68990\n",
            "Epoch 452/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.68990\n",
            "Epoch 453/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.68990\n",
            "Epoch 454/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.68990\n",
            "Epoch 455/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.68990\n",
            "Epoch 456/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.68990\n",
            "Epoch 457/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.68990\n",
            "Epoch 458/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.68990\n",
            "Epoch 459/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.68990\n",
            "Epoch 460/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.68990\n",
            "Epoch 461/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.68990\n",
            "Epoch 462/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.68990\n",
            "Epoch 463/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.68990\n",
            "Epoch 464/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.68990\n",
            "Epoch 465/750\n",
            "9809/9809 [==============================] - 0s 46us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.68990\n",
            "Epoch 466/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.68990\n",
            "Epoch 467/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.68990\n",
            "Epoch 468/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.68990\n",
            "Epoch 469/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.68990\n",
            "Epoch 470/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.68990\n",
            "Epoch 471/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.68990\n",
            "Epoch 472/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.68990\n",
            "Epoch 473/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.68990\n",
            "Epoch 474/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.68990\n",
            "Epoch 475/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.68990\n",
            "Epoch 476/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00476: ReduceLROnPlateau reducing learning rate to 0.000135085187503.\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.68990\n",
            "Epoch 477/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.68990\n",
            "Epoch 478/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.68990\n",
            "Epoch 479/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.68990\n",
            "Epoch 480/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.68990\n",
            "Epoch 481/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.68990\n",
            "Epoch 482/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.68990\n",
            "Epoch 483/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.68990\n",
            "Epoch 484/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.68990\n",
            "Epoch 485/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.68990\n",
            "Epoch 486/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.68990\n",
            "Epoch 487/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.68990\n",
            "Epoch 488/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.68990\n",
            "Epoch 489/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.68990\n",
            "Epoch 490/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.68990\n",
            "Epoch 491/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.68990\n",
            "Epoch 492/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.68990\n",
            "Epoch 493/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.68990\n",
            "Epoch 494/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.68990\n",
            "Epoch 495/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.68990\n",
            "Epoch 496/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.68990\n",
            "Epoch 497/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.68990\n",
            "Epoch 498/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.68990\n",
            "Epoch 499/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.68990\n",
            "Epoch 500/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.68990\n",
            "Epoch 501/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00501: ReduceLROnPlateau reducing learning rate to 0.000121576663514.\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.68990\n",
            "Epoch 502/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.68990\n",
            "Epoch 503/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.68990\n",
            "Epoch 504/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.68990\n",
            "Epoch 505/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.68990\n",
            "Epoch 506/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.68990\n",
            "Epoch 507/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.68990\n",
            "Epoch 508/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.68990\n",
            "Epoch 509/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.68990\n",
            "Epoch 510/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.68990\n",
            "Epoch 511/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.68990\n",
            "Epoch 512/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.68990\n",
            "Epoch 513/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.68990\n",
            "Epoch 514/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.68990\n",
            "Epoch 515/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.68990\n",
            "Epoch 516/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.68990\n",
            "Epoch 517/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.68990\n",
            "Epoch 518/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.68990\n",
            "Epoch 519/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.68990\n",
            "Epoch 520/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.68990\n",
            "Epoch 521/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.68990\n",
            "Epoch 522/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.68990\n",
            "Epoch 523/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.68990\n",
            "Epoch 524/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.68990\n",
            "Epoch 525/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.68990\n",
            "Epoch 526/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00526: ReduceLROnPlateau reducing learning rate to 0.000109418994543.\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.68990\n",
            "Epoch 527/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.68990\n",
            "Epoch 528/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.68990\n",
            "Epoch 529/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.68990\n",
            "Epoch 530/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.68990\n",
            "Epoch 531/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.68990\n",
            "Epoch 532/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.68990\n",
            "Epoch 533/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.68990\n",
            "Epoch 534/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.68990\n",
            "Epoch 535/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.68990\n",
            "Epoch 536/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.68990\n",
            "Epoch 537/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.68990\n",
            "Epoch 538/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.68990\n",
            "Epoch 539/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.68990\n",
            "Epoch 540/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.68990\n",
            "Epoch 541/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.68990\n",
            "Epoch 542/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.68990\n",
            "Epoch 543/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.68990\n",
            "Epoch 544/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.68990\n",
            "Epoch 545/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.68990\n",
            "Epoch 546/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.68990\n",
            "Epoch 547/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.68990\n",
            "Epoch 548/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.68990\n",
            "Epoch 549/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.68990\n",
            "Epoch 550/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.68990\n",
            "Epoch 551/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00551: ReduceLROnPlateau reducing learning rate to 9.84770944342e-05.\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.68990\n",
            "Epoch 552/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.68990\n",
            "Epoch 553/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.68990\n",
            "Epoch 554/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.68990\n",
            "Epoch 555/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.68990\n",
            "Epoch 556/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.68990\n",
            "Epoch 557/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.68990\n",
            "Epoch 558/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.68990\n",
            "Epoch 559/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.68990\n",
            "Epoch 560/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.68990\n",
            "Epoch 561/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.68990\n",
            "Epoch 562/750\n",
            "9809/9809 [==============================] - 0s 47us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.68990\n",
            "Epoch 563/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.68990\n",
            "Epoch 564/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.68990\n",
            "Epoch 565/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.68990\n",
            "Epoch 566/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.68990\n",
            "Epoch 567/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.68990\n",
            "Epoch 568/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.68990\n",
            "Epoch 569/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.68990\n",
            "Epoch 570/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.68990\n",
            "Epoch 571/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.68990\n",
            "Epoch 572/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.68990\n",
            "Epoch 573/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.68990\n",
            "Epoch 574/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.68990\n",
            "Epoch 575/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.68990\n",
            "Epoch 576/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00576: ReduceLROnPlateau reducing learning rate to 8.86293863005e-05.\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.68990\n",
            "Epoch 577/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.68990\n",
            "Epoch 578/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.68990\n",
            "Epoch 579/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.68990\n",
            "Epoch 580/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.68990\n",
            "Epoch 581/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.68990\n",
            "Epoch 582/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.68990\n",
            "Epoch 583/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.68990\n",
            "Epoch 584/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.68990\n",
            "Epoch 585/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6900 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.68990\n",
            "Epoch 586/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.68990\n",
            "Epoch 587/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.68990\n",
            "Epoch 588/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.68990\n",
            "Epoch 589/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.68990\n",
            "Epoch 590/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.68990\n",
            "Epoch 591/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.68990\n",
            "Epoch 592/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.68990\n",
            "Epoch 593/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.68990\n",
            "Epoch 594/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.68990\n",
            "Epoch 595/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.68990\n",
            "Epoch 596/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.68990\n",
            "Epoch 597/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.68990\n",
            "Epoch 598/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.68990\n",
            "Epoch 599/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.68990\n",
            "Epoch 600/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.68990\n",
            "Epoch 601/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00601: ReduceLROnPlateau reducing learning rate to 7.97664470156e-05.\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.68990\n",
            "Epoch 602/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.68990\n",
            "Epoch 603/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.68990\n",
            "Epoch 604/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.68990\n",
            "Epoch 605/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.68990\n",
            "Epoch 606/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.68990\n",
            "Epoch 607/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.68990\n",
            "Epoch 608/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.68990\n",
            "Epoch 609/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.68990\n",
            "Epoch 610/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.68990\n",
            "Epoch 611/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.68990\n",
            "Epoch 612/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.68990\n",
            "Epoch 613/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.68990\n",
            "Epoch 614/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.68990\n",
            "Epoch 615/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.68990\n",
            "Epoch 616/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.68990\n",
            "Epoch 617/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.68990\n",
            "Epoch 618/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.68990\n",
            "Epoch 619/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.68990\n",
            "Epoch 620/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.68990\n",
            "Epoch 621/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.68990\n",
            "Epoch 622/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.68990\n",
            "Epoch 623/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.68990\n",
            "Epoch 624/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.68990\n",
            "Epoch 625/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.68990\n",
            "Epoch 626/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00626: ReduceLROnPlateau reducing learning rate to 7.1789802314e-05.\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.68990\n",
            "Epoch 627/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.68990\n",
            "Epoch 628/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.68990\n",
            "Epoch 629/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.68990\n",
            "Epoch 630/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.68990\n",
            "Epoch 631/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.68990\n",
            "Epoch 632/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.68990\n",
            "Epoch 633/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.68990\n",
            "Epoch 634/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.68990\n",
            "Epoch 635/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.68990\n",
            "Epoch 636/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.68990\n",
            "Epoch 637/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.68990\n",
            "Epoch 638/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.68990\n",
            "Epoch 639/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.68990\n",
            "Epoch 640/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.68990\n",
            "Epoch 641/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.68990\n",
            "Epoch 642/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.68990\n",
            "Epoch 643/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.68990\n",
            "Epoch 644/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.68990\n",
            "Epoch 645/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.68990\n",
            "Epoch 646/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.68990\n",
            "Epoch 647/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.68990\n",
            "Epoch 648/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.68990\n",
            "Epoch 649/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.68990\n",
            "Epoch 650/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.68990\n",
            "Epoch 651/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00651: ReduceLROnPlateau reducing learning rate to 6.46108201181e-05.\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.68990\n",
            "Epoch 652/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.68990\n",
            "Epoch 653/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.68990\n",
            "Epoch 654/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.68990\n",
            "Epoch 655/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.68990\n",
            "Epoch 656/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.68990\n",
            "Epoch 657/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.68990\n",
            "Epoch 658/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.68990\n",
            "Epoch 659/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.68990\n",
            "Epoch 660/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.68990\n",
            "Epoch 661/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.68990\n",
            "Epoch 662/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.68990\n",
            "Epoch 663/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.68990\n",
            "Epoch 664/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.68990\n",
            "Epoch 665/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.68990\n",
            "Epoch 666/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.68990\n",
            "Epoch 667/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.68990\n",
            "Epoch 668/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.68990\n",
            "Epoch 669/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.68990\n",
            "Epoch 670/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.68990\n",
            "Epoch 671/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.68990\n",
            "Epoch 672/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.68990\n",
            "Epoch 673/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.68990\n",
            "Epoch 674/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.68990\n",
            "Epoch 675/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.68990\n",
            "Epoch 676/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00676: ReduceLROnPlateau reducing learning rate to 5.81497413805e-05.\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.68990\n",
            "Epoch 677/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.68990\n",
            "Epoch 678/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.68990\n",
            "Epoch 679/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.68990\n",
            "Epoch 680/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.68990\n",
            "Epoch 681/750\n",
            "9809/9809 [==============================] - 0s 49us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.68990\n",
            "Epoch 682/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.68990\n",
            "Epoch 683/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.68990\n",
            "Epoch 684/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.68990\n",
            "Epoch 685/750\n",
            "9809/9809 [==============================] - 0s 50us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.68990\n",
            "Epoch 686/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.68990\n",
            "Epoch 687/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.68990\n",
            "Epoch 688/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.68990\n",
            "Epoch 689/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.68990\n",
            "Epoch 690/750\n",
            "9809/9809 [==============================] - 1s 57us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.68990\n",
            "Epoch 691/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.68990\n",
            "Epoch 692/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.68990\n",
            "Epoch 693/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.68990\n",
            "Epoch 694/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.68990\n",
            "Epoch 695/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.68990\n",
            "Epoch 696/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.68990\n",
            "Epoch 697/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.68990\n",
            "Epoch 698/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.68990\n",
            "Epoch 699/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.68990\n",
            "Epoch 700/750\n",
            "9809/9809 [==============================] - 0s 48us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.68990\n",
            "Epoch 701/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00701: ReduceLROnPlateau reducing learning rate to 5.23347665876e-05.\n",
            "\n",
            "Epoch 00701: val_loss did not improve from 0.68990\n",
            "Epoch 702/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00702: val_loss did not improve from 0.68990\n",
            "Epoch 703/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00703: val_loss did not improve from 0.68990\n",
            "Epoch 704/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00704: val_loss did not improve from 0.68990\n",
            "Epoch 705/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00705: val_loss did not improve from 0.68990\n",
            "Epoch 706/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00706: val_loss did not improve from 0.68990\n",
            "Epoch 707/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00707: val_loss did not improve from 0.68990\n",
            "Epoch 708/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00708: val_loss did not improve from 0.68990\n",
            "Epoch 709/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00709: val_loss did not improve from 0.68990\n",
            "Epoch 710/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00710: val_loss did not improve from 0.68990\n",
            "Epoch 711/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00711: val_loss did not improve from 0.68990\n",
            "Epoch 712/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00712: val_loss did not improve from 0.68990\n",
            "Epoch 713/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00713: val_loss did not improve from 0.68990\n",
            "Epoch 714/750\n",
            "9809/9809 [==============================] - 0s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00714: val_loss did not improve from 0.68990\n",
            "Epoch 715/750\n",
            "9809/9809 [==============================] - 1s 58us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00715: val_loss did not improve from 0.68990\n",
            "Epoch 716/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00716: val_loss did not improve from 0.68990\n",
            "Epoch 717/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00717: val_loss did not improve from 0.68990\n",
            "Epoch 718/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00718: val_loss did not improve from 0.68990\n",
            "Epoch 719/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00719: val_loss did not improve from 0.68990\n",
            "Epoch 720/750\n",
            "9809/9809 [==============================] - 1s 51us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00720: val_loss did not improve from 0.68990\n",
            "Epoch 721/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00721: val_loss did not improve from 0.68990\n",
            "Epoch 722/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00722: val_loss did not improve from 0.68990\n",
            "Epoch 723/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00723: val_loss did not improve from 0.68990\n",
            "Epoch 724/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00724: val_loss did not improve from 0.68990\n",
            "Epoch 725/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00725: val_loss did not improve from 0.68990\n",
            "Epoch 726/750\n",
            "9809/9809 [==============================] - 1s 57us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00726: ReduceLROnPlateau reducing learning rate to 4.71012896014e-05.\n",
            "\n",
            "Epoch 00726: val_loss did not improve from 0.68990\n",
            "Epoch 727/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00727: val_loss did not improve from 0.68990\n",
            "Epoch 728/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00728: val_loss did not improve from 0.68990\n",
            "Epoch 729/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00729: val_loss did not improve from 0.68990\n",
            "Epoch 730/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6905 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00730: val_loss did not improve from 0.68990\n",
            "Epoch 731/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00731: val_loss did not improve from 0.68990\n",
            "Epoch 732/750\n",
            "9809/9809 [==============================] - 1s 57us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00732: val_loss did not improve from 0.68990\n",
            "Epoch 733/750\n",
            "9809/9809 [==============================] - 1s 58us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00733: val_loss did not improve from 0.68990\n",
            "Epoch 734/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00734: val_loss did not improve from 0.68990\n",
            "Epoch 735/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00735: val_loss did not improve from 0.68990\n",
            "Epoch 736/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00736: val_loss did not improve from 0.68990\n",
            "Epoch 737/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00737: val_loss did not improve from 0.68990\n",
            "Epoch 738/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00738: val_loss did not improve from 0.68990\n",
            "Epoch 739/750\n",
            "9809/9809 [==============================] - 1s 55us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00739: val_loss did not improve from 0.68990\n",
            "Epoch 740/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00740: val_loss did not improve from 0.68990\n",
            "Epoch 741/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00741: val_loss did not improve from 0.68990\n",
            "Epoch 742/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00742: val_loss did not improve from 0.68990\n",
            "Epoch 743/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00743: val_loss did not improve from 0.68990\n",
            "Epoch 744/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00744: val_loss did not improve from 0.68990\n",
            "Epoch 745/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00745: val_loss did not improve from 0.68990\n",
            "Epoch 746/750\n",
            "9809/9809 [==============================] - 1s 54us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00746: val_loss did not improve from 0.68990\n",
            "Epoch 747/750\n",
            "9809/9809 [==============================] - 1s 53us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00747: val_loss did not improve from 0.68990\n",
            "Epoch 748/750\n",
            "9809/9809 [==============================] - 1s 52us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00748: val_loss did not improve from 0.68990\n",
            "Epoch 749/750\n",
            "9809/9809 [==============================] - 1s 58us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00749: val_loss did not improve from 0.68990\n",
            "Epoch 750/750\n",
            "9809/9809 [==============================] - 1s 56us/step - loss: 0.6904 - accuracy: 0.5370 - val_loss: 0.6899 - val_accuracy: 0.5404\n",
            "\n",
            "Epoch 00750: val_loss did not improve from 0.68990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fOSlmnbSdWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "outputId": "3bcfbdbe-e15b-4daa-b763-c94bd9638d69"
      },
      "source": [
        "plt.figure()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='best')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print('Accuracy: ', history.history['val_accuracy'][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "('Accuracy: ', 0.5403669476509094)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VsAxopwWSdWf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "outputId": "a36edb74-cf21-44ff-9b7b-6424e57562a0"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "pred = model.predict(np.array(X_test))\n",
        "C = confusion_matrix([np.argmax(y) for y in Y_test], [np.argmax(y) for y in pred])\n",
        "\n",
        "print C / C.astype(np.float).sum(axis=1)\n",
        "\n",
        "FROM = 0\n",
        "TO = FROM + 500\n",
        "\n",
        "original = Y_test[FROM:TO]\n",
        "predicted = pred[FROM:TO] \n",
        "\n",
        "plt.plot(original, color='black', label = 'Original data')\n",
        "plt.plot(predicted, color='blue', label = 'Predicted data')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Actual and predicted from point %d to point %d of test set' % (FROM, TO))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.84210526 0.20168067]\n",
            " [0.66447368 0.1512605 ]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBt0EqIoSdWf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}